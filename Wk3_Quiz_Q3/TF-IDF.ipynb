{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75a94574-11e8-401e-af69-5f84a907813f",
   "metadata": {},
   "source": [
    "# Approach\n",
    "\n",
    "1. Load datasets, change multiclass to binary class (positive or negative only), limit to text column\n",
    "2. Sklearn pipeline: encode text (sentiment category and tweet), tf-idf\n",
    "3. Torch pipeline: MLP with sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb09831-0d25-4f42-8734-4b3eaa0cf9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55dd715f-542e-40ac-9a87-9740482eb9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9519d44a-6b7d-453c-bd4e-690a93c1bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transformer(df):\n",
    "    '''\n",
    "    Change multiclass to binary class: positive or negative tweets only\n",
    "    Apply sklearn encoding on Sentiment column\n",
    "    \n",
    "    Param: Dataframe to transform\n",
    "    Returns: Transformed dataframe\n",
    "    '''\n",
    "    df['Sentiment'] = df['Sentiment'].map({'Positive':'Positive', 'Extremely Positive':'Positive', \n",
    "                                           'Negative':'Negative', 'Extremely Negative':'Negative',\n",
    "                                           'Neutral':'Positive'\n",
    "                                          })\n",
    "    df = df.drop(['UserName','ScreenName','Location','TweetAt'], axis=1)\n",
    "    \n",
    "    # Encode sentiment values\n",
    "    df_le = LabelEncoder().fit(df['Sentiment'])\n",
    "    df['encoded_sentiment'] = df_le.transform(df['Sentiment'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6ab7a2c-574d-4770-9fd9-6da8460b273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./Data/Corona_NLP_train.csv')\n",
    "df_test = pd.read_csv('./Data/Corona_NLP_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3031c1f3-ca1b-4380-b2ff-c87def222402",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data_transformer(df_train)\n",
    "df_test = data_transformer(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a1af5-d676-403e-95fd-d552cc551ab6",
   "metadata": {},
   "source": [
    "### Sklearn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2565d518-eb2f-4d04-868a-1c143304ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = df_train['OriginalTweet'], df_test['OriginalTweet'], df_train['encoded_sentiment'], df_test['encoded_sentiment']\n",
    "\n",
    "# Perform tf-idf on OriginalTweets\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "x_train = tf_idf.fit_transform(x_train)\n",
    "x_test = tf_idf.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b40ab35-57a1-40c6-9836-954c3a915566",
   "metadata": {},
   "source": [
    "### PyTorch pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce8c790c-e5b5-4c5c-a4d4-01e68bd1c906",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(scipy.sparse.csr_matrix.todense(x_train)).float()\n",
    "x_test = torch.tensor(scipy.sparse.csr_matrix.todense(x_test)).float()\n",
    "\n",
    "y_train = torch.tensor(y_train.values).long()\n",
    "y_test = torch.tensor(y_test.values).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8afb918-9973-48d1-afdc-748ca7af2df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_test_shape = int((x_test.shape[1]*x_test.shape[0]) / 18) # You can try: 6, 9, 18\n",
    "\n",
    "# x_test = x_test.reshape(18, new_test_shape)\n",
    "\n",
    "# x_test = x_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b71a715b-bdf4-4e40-b96f-fc2c1a7e264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(x_train.shape[1],64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(0.1),\n",
    "                      nn.Linear(64, 2), # There are 2 output classes = +ve & -ve\n",
    "                      nn.Sigmoid())  #nn.LogSoftmax(dim=1)) # The tutorial website used logsoftmax for binary class\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.CrossEntropyLoss() #NLLLoss() # The tutorial website used NLLLoss for binary class\n",
    "\n",
    "# Forward pass, get our logits\n",
    "output = model(x_train)\n",
    "\n",
    "# Calculate the loss with the logits and the labels\n",
    "loss = criterion(output, y_train)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f079ad4-c418-42fc-a76a-997acd85abb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200..  Training Loss: 0.699..  Test Loss: 0.696..  Test Accuracy: 0.430\n",
      "Epoch: 2/200..  Training Loss: 0.698..  Test Loss: 0.695..  Test Accuracy: 0.430\n",
      "Epoch: 3/200..  Training Loss: 0.697..  Test Loss: 0.695..  Test Accuracy: 0.430\n",
      "Epoch: 4/200..  Training Loss: 0.695..  Test Loss: 0.694..  Test Accuracy: 0.430\n",
      "Epoch: 5/200..  Training Loss: 0.694..  Test Loss: 0.693..  Test Accuracy: 0.460\n",
      "Epoch: 6/200..  Training Loss: 0.692..  Test Loss: 0.692..  Test Accuracy: 0.612\n",
      "Epoch: 7/200..  Training Loss: 0.690..  Test Loss: 0.691..  Test Accuracy: 0.635\n",
      "Epoch: 8/200..  Training Loss: 0.688..  Test Loss: 0.690..  Test Accuracy: 0.628\n",
      "Epoch: 9/200..  Training Loss: 0.686..  Test Loss: 0.689..  Test Accuracy: 0.625\n",
      "Epoch: 10/200..  Training Loss: 0.683..  Test Loss: 0.687..  Test Accuracy: 0.621\n",
      "Epoch: 11/200..  Training Loss: 0.681..  Test Loss: 0.686..  Test Accuracy: 0.619\n",
      "Epoch: 12/200..  Training Loss: 0.678..  Test Loss: 0.685..  Test Accuracy: 0.618\n",
      "Epoch: 13/200..  Training Loss: 0.675..  Test Loss: 0.683..  Test Accuracy: 0.615\n",
      "Epoch: 14/200..  Training Loss: 0.672..  Test Loss: 0.682..  Test Accuracy: 0.613\n",
      "Epoch: 15/200..  Training Loss: 0.669..  Test Loss: 0.681..  Test Accuracy: 0.607\n",
      "Epoch: 16/200..  Training Loss: 0.666..  Test Loss: 0.679..  Test Accuracy: 0.604\n",
      "Epoch: 17/200..  Training Loss: 0.662..  Test Loss: 0.678..  Test Accuracy: 0.603\n",
      "Epoch: 18/200..  Training Loss: 0.659..  Test Loss: 0.676..  Test Accuracy: 0.601\n",
      "Epoch: 19/200..  Training Loss: 0.655..  Test Loss: 0.675..  Test Accuracy: 0.600\n",
      "Epoch: 20/200..  Training Loss: 0.651..  Test Loss: 0.673..  Test Accuracy: 0.598\n",
      "Epoch: 21/200..  Training Loss: 0.647..  Test Loss: 0.671..  Test Accuracy: 0.598\n",
      "Epoch: 22/200..  Training Loss: 0.643..  Test Loss: 0.670..  Test Accuracy: 0.600\n",
      "Epoch: 23/200..  Training Loss: 0.639..  Test Loss: 0.668..  Test Accuracy: 0.601\n",
      "Epoch: 24/200..  Training Loss: 0.635..  Test Loss: 0.666..  Test Accuracy: 0.602\n",
      "Epoch: 25/200..  Training Loss: 0.631..  Test Loss: 0.665..  Test Accuracy: 0.603\n",
      "Epoch: 26/200..  Training Loss: 0.626..  Test Loss: 0.663..  Test Accuracy: 0.604\n",
      "Epoch: 27/200..  Training Loss: 0.622..  Test Loss: 0.661..  Test Accuracy: 0.605\n",
      "Epoch: 28/200..  Training Loss: 0.617..  Test Loss: 0.659..  Test Accuracy: 0.608\n",
      "Epoch: 29/200..  Training Loss: 0.612..  Test Loss: 0.658..  Test Accuracy: 0.610\n",
      "Epoch: 30/200..  Training Loss: 0.608..  Test Loss: 0.656..  Test Accuracy: 0.613\n",
      "Epoch: 31/200..  Training Loss: 0.603..  Test Loss: 0.654..  Test Accuracy: 0.617\n",
      "Epoch: 32/200..  Training Loss: 0.598..  Test Loss: 0.652..  Test Accuracy: 0.620\n",
      "Epoch: 33/200..  Training Loss: 0.593..  Test Loss: 0.650..  Test Accuracy: 0.623\n",
      "Epoch: 34/200..  Training Loss: 0.588..  Test Loss: 0.648..  Test Accuracy: 0.628\n",
      "Epoch: 35/200..  Training Loss: 0.583..  Test Loss: 0.646..  Test Accuracy: 0.633\n",
      "Epoch: 36/200..  Training Loss: 0.578..  Test Loss: 0.644..  Test Accuracy: 0.637\n",
      "Epoch: 37/200..  Training Loss: 0.573..  Test Loss: 0.642..  Test Accuracy: 0.642\n",
      "Epoch: 38/200..  Training Loss: 0.568..  Test Loss: 0.640..  Test Accuracy: 0.646\n",
      "Epoch: 39/200..  Training Loss: 0.563..  Test Loss: 0.638..  Test Accuracy: 0.651\n",
      "Epoch: 40/200..  Training Loss: 0.558..  Test Loss: 0.636..  Test Accuracy: 0.659\n",
      "Epoch: 41/200..  Training Loss: 0.553..  Test Loss: 0.634..  Test Accuracy: 0.667\n",
      "Epoch: 42/200..  Training Loss: 0.548..  Test Loss: 0.631..  Test Accuracy: 0.675\n",
      "Epoch: 43/200..  Training Loss: 0.543..  Test Loss: 0.629..  Test Accuracy: 0.680\n",
      "Epoch: 44/200..  Training Loss: 0.538..  Test Loss: 0.627..  Test Accuracy: 0.683\n",
      "Epoch: 45/200..  Training Loss: 0.533..  Test Loss: 0.625..  Test Accuracy: 0.689\n",
      "Epoch: 46/200..  Training Loss: 0.528..  Test Loss: 0.622..  Test Accuracy: 0.695\n",
      "Epoch: 47/200..  Training Loss: 0.523..  Test Loss: 0.620..  Test Accuracy: 0.700\n",
      "Epoch: 48/200..  Training Loss: 0.518..  Test Loss: 0.618..  Test Accuracy: 0.706\n",
      "Epoch: 49/200..  Training Loss: 0.513..  Test Loss: 0.616..  Test Accuracy: 0.711\n",
      "Epoch: 50/200..  Training Loss: 0.509..  Test Loss: 0.614..  Test Accuracy: 0.714\n",
      "Epoch: 51/200..  Training Loss: 0.504..  Test Loss: 0.612..  Test Accuracy: 0.713\n",
      "Epoch: 52/200..  Training Loss: 0.500..  Test Loss: 0.610..  Test Accuracy: 0.711\n",
      "Epoch: 53/200..  Training Loss: 0.496..  Test Loss: 0.609..  Test Accuracy: 0.712\n",
      "Epoch: 54/200..  Training Loss: 0.491..  Test Loss: 0.607..  Test Accuracy: 0.713\n",
      "Epoch: 55/200..  Training Loss: 0.487..  Test Loss: 0.605..  Test Accuracy: 0.717\n",
      "Epoch: 56/200..  Training Loss: 0.483..  Test Loss: 0.604..  Test Accuracy: 0.719\n",
      "Epoch: 57/200..  Training Loss: 0.479..  Test Loss: 0.602..  Test Accuracy: 0.719\n",
      "Epoch: 58/200..  Training Loss: 0.475..  Test Loss: 0.601..  Test Accuracy: 0.720\n",
      "Epoch: 59/200..  Training Loss: 0.471..  Test Loss: 0.599..  Test Accuracy: 0.720\n",
      "Epoch: 60/200..  Training Loss: 0.468..  Test Loss: 0.598..  Test Accuracy: 0.720\n",
      "Epoch: 61/200..  Training Loss: 0.464..  Test Loss: 0.597..  Test Accuracy: 0.720\n",
      "Epoch: 62/200..  Training Loss: 0.461..  Test Loss: 0.595..  Test Accuracy: 0.721\n",
      "Epoch: 63/200..  Training Loss: 0.457..  Test Loss: 0.594..  Test Accuracy: 0.721\n",
      "Epoch: 64/200..  Training Loss: 0.454..  Test Loss: 0.593..  Test Accuracy: 0.720\n",
      "Epoch: 65/200..  Training Loss: 0.451..  Test Loss: 0.592..  Test Accuracy: 0.722\n",
      "Epoch: 66/200..  Training Loss: 0.447..  Test Loss: 0.591..  Test Accuracy: 0.723\n",
      "Epoch: 67/200..  Training Loss: 0.444..  Test Loss: 0.590..  Test Accuracy: 0.724\n",
      "Epoch: 68/200..  Training Loss: 0.441..  Test Loss: 0.588..  Test Accuracy: 0.725\n",
      "Epoch: 69/200..  Training Loss: 0.438..  Test Loss: 0.587..  Test Accuracy: 0.727\n",
      "Epoch: 70/200..  Training Loss: 0.436..  Test Loss: 0.586..  Test Accuracy: 0.728\n",
      "Epoch: 71/200..  Training Loss: 0.433..  Test Loss: 0.585..  Test Accuracy: 0.728\n",
      "Epoch: 72/200..  Training Loss: 0.430..  Test Loss: 0.585..  Test Accuracy: 0.729\n",
      "Epoch: 73/200..  Training Loss: 0.428..  Test Loss: 0.584..  Test Accuracy: 0.730\n",
      "Epoch: 74/200..  Training Loss: 0.425..  Test Loss: 0.583..  Test Accuracy: 0.730\n",
      "Epoch: 75/200..  Training Loss: 0.423..  Test Loss: 0.582..  Test Accuracy: 0.731\n",
      "Epoch: 76/200..  Training Loss: 0.420..  Test Loss: 0.581..  Test Accuracy: 0.732\n",
      "Epoch: 77/200..  Training Loss: 0.418..  Test Loss: 0.580..  Test Accuracy: 0.732\n",
      "Epoch: 78/200..  Training Loss: 0.416..  Test Loss: 0.580..  Test Accuracy: 0.732\n",
      "Epoch: 79/200..  Training Loss: 0.414..  Test Loss: 0.579..  Test Accuracy: 0.732\n",
      "Epoch: 80/200..  Training Loss: 0.411..  Test Loss: 0.578..  Test Accuracy: 0.732\n",
      "Epoch: 81/200..  Training Loss: 0.410..  Test Loss: 0.577..  Test Accuracy: 0.734\n",
      "Epoch: 82/200..  Training Loss: 0.407..  Test Loss: 0.577..  Test Accuracy: 0.735\n",
      "Epoch: 83/200..  Training Loss: 0.406..  Test Loss: 0.576..  Test Accuracy: 0.735\n",
      "Epoch: 84/200..  Training Loss: 0.404..  Test Loss: 0.575..  Test Accuracy: 0.736\n",
      "Epoch: 85/200..  Training Loss: 0.402..  Test Loss: 0.575..  Test Accuracy: 0.736\n",
      "Epoch: 86/200..  Training Loss: 0.400..  Test Loss: 0.574..  Test Accuracy: 0.736\n",
      "Epoch: 87/200..  Training Loss: 0.398..  Test Loss: 0.573..  Test Accuracy: 0.736\n",
      "Epoch: 88/200..  Training Loss: 0.397..  Test Loss: 0.573..  Test Accuracy: 0.737\n",
      "Epoch: 89/200..  Training Loss: 0.395..  Test Loss: 0.572..  Test Accuracy: 0.738\n",
      "Epoch: 90/200..  Training Loss: 0.393..  Test Loss: 0.572..  Test Accuracy: 0.741\n",
      "Epoch: 91/200..  Training Loss: 0.392..  Test Loss: 0.571..  Test Accuracy: 0.740\n",
      "Epoch: 92/200..  Training Loss: 0.390..  Test Loss: 0.571..  Test Accuracy: 0.740\n",
      "Epoch: 93/200..  Training Loss: 0.389..  Test Loss: 0.570..  Test Accuracy: 0.740\n",
      "Epoch: 94/200..  Training Loss: 0.387..  Test Loss: 0.570..  Test Accuracy: 0.741\n",
      "Epoch: 95/200..  Training Loss: 0.386..  Test Loss: 0.569..  Test Accuracy: 0.742\n",
      "Epoch: 96/200..  Training Loss: 0.385..  Test Loss: 0.569..  Test Accuracy: 0.741\n",
      "Epoch: 97/200..  Training Loss: 0.383..  Test Loss: 0.568..  Test Accuracy: 0.742\n",
      "Epoch: 98/200..  Training Loss: 0.382..  Test Loss: 0.568..  Test Accuracy: 0.743\n",
      "Epoch: 99/200..  Training Loss: 0.381..  Test Loss: 0.568..  Test Accuracy: 0.743\n",
      "Epoch: 100/200..  Training Loss: 0.380..  Test Loss: 0.567..  Test Accuracy: 0.743\n",
      "Epoch: 101/200..  Training Loss: 0.379..  Test Loss: 0.567..  Test Accuracy: 0.742\n",
      "Epoch: 102/200..  Training Loss: 0.377..  Test Loss: 0.566..  Test Accuracy: 0.743\n",
      "Epoch: 103/200..  Training Loss: 0.376..  Test Loss: 0.566..  Test Accuracy: 0.744\n",
      "Epoch: 104/200..  Training Loss: 0.375..  Test Loss: 0.566..  Test Accuracy: 0.743\n",
      "Epoch: 105/200..  Training Loss: 0.374..  Test Loss: 0.565..  Test Accuracy: 0.744\n",
      "Epoch: 106/200..  Training Loss: 0.373..  Test Loss: 0.565..  Test Accuracy: 0.743\n",
      "Epoch: 107/200..  Training Loss: 0.372..  Test Loss: 0.565..  Test Accuracy: 0.743\n",
      "Epoch: 108/200..  Training Loss: 0.371..  Test Loss: 0.564..  Test Accuracy: 0.743\n",
      "Epoch: 109/200..  Training Loss: 0.370..  Test Loss: 0.564..  Test Accuracy: 0.743\n",
      "Epoch: 110/200..  Training Loss: 0.370..  Test Loss: 0.564..  Test Accuracy: 0.744\n",
      "Epoch: 111/200..  Training Loss: 0.369..  Test Loss: 0.563..  Test Accuracy: 0.744\n",
      "Epoch: 112/200..  Training Loss: 0.368..  Test Loss: 0.563..  Test Accuracy: 0.744\n",
      "Epoch: 113/200..  Training Loss: 0.367..  Test Loss: 0.563..  Test Accuracy: 0.744\n",
      "Epoch: 114/200..  Training Loss: 0.366..  Test Loss: 0.563..  Test Accuracy: 0.744\n",
      "Epoch: 115/200..  Training Loss: 0.365..  Test Loss: 0.562..  Test Accuracy: 0.744\n",
      "Epoch: 116/200..  Training Loss: 0.364..  Test Loss: 0.562..  Test Accuracy: 0.744\n",
      "Epoch: 117/200..  Training Loss: 0.364..  Test Loss: 0.562..  Test Accuracy: 0.743\n",
      "Epoch: 118/200..  Training Loss: 0.363..  Test Loss: 0.562..  Test Accuracy: 0.743\n",
      "Epoch: 119/200..  Training Loss: 0.362..  Test Loss: 0.561..  Test Accuracy: 0.743\n",
      "Epoch: 120/200..  Training Loss: 0.361..  Test Loss: 0.561..  Test Accuracy: 0.744\n",
      "Epoch: 121/200..  Training Loss: 0.361..  Test Loss: 0.561..  Test Accuracy: 0.744\n",
      "Epoch: 122/200..  Training Loss: 0.360..  Test Loss: 0.561..  Test Accuracy: 0.744\n",
      "Epoch: 123/200..  Training Loss: 0.359..  Test Loss: 0.561..  Test Accuracy: 0.744\n",
      "Epoch: 124/200..  Training Loss: 0.359..  Test Loss: 0.560..  Test Accuracy: 0.745\n",
      "Epoch: 125/200..  Training Loss: 0.358..  Test Loss: 0.560..  Test Accuracy: 0.745\n",
      "Epoch: 126/200..  Training Loss: 0.357..  Test Loss: 0.560..  Test Accuracy: 0.746\n",
      "Epoch: 127/200..  Training Loss: 0.357..  Test Loss: 0.560..  Test Accuracy: 0.745\n",
      "Epoch: 128/200..  Training Loss: 0.356..  Test Loss: 0.560..  Test Accuracy: 0.745\n",
      "Epoch: 129/200..  Training Loss: 0.356..  Test Loss: 0.560..  Test Accuracy: 0.745\n",
      "Epoch: 130/200..  Training Loss: 0.355..  Test Loss: 0.559..  Test Accuracy: 0.745\n",
      "Epoch: 131/200..  Training Loss: 0.355..  Test Loss: 0.559..  Test Accuracy: 0.745\n",
      "Epoch: 132/200..  Training Loss: 0.354..  Test Loss: 0.559..  Test Accuracy: 0.744\n",
      "Epoch: 133/200..  Training Loss: 0.353..  Test Loss: 0.559..  Test Accuracy: 0.744\n",
      "Epoch: 134/200..  Training Loss: 0.353..  Test Loss: 0.559..  Test Accuracy: 0.744\n",
      "Epoch: 135/200..  Training Loss: 0.352..  Test Loss: 0.559..  Test Accuracy: 0.743\n",
      "Epoch: 136/200..  Training Loss: 0.352..  Test Loss: 0.559..  Test Accuracy: 0.743\n",
      "Epoch: 137/200..  Training Loss: 0.351..  Test Loss: 0.558..  Test Accuracy: 0.743\n",
      "Epoch: 138/200..  Training Loss: 0.351..  Test Loss: 0.558..  Test Accuracy: 0.743\n",
      "Epoch: 139/200..  Training Loss: 0.350..  Test Loss: 0.558..  Test Accuracy: 0.744\n",
      "Epoch: 140/200..  Training Loss: 0.350..  Test Loss: 0.558..  Test Accuracy: 0.743\n",
      "Epoch: 141/200..  Training Loss: 0.349..  Test Loss: 0.558..  Test Accuracy: 0.744\n",
      "Epoch: 142/200..  Training Loss: 0.349..  Test Loss: 0.558..  Test Accuracy: 0.745\n",
      "Epoch: 143/200..  Training Loss: 0.349..  Test Loss: 0.558..  Test Accuracy: 0.744\n",
      "Epoch: 144/200..  Training Loss: 0.348..  Test Loss: 0.558..  Test Accuracy: 0.745\n",
      "Epoch: 145/200..  Training Loss: 0.348..  Test Loss: 0.558..  Test Accuracy: 0.745\n",
      "Epoch: 146/200..  Training Loss: 0.347..  Test Loss: 0.558..  Test Accuracy: 0.745\n",
      "Epoch: 147/200..  Training Loss: 0.347..  Test Loss: 0.557..  Test Accuracy: 0.745\n",
      "Epoch: 148/200..  Training Loss: 0.346..  Test Loss: 0.557..  Test Accuracy: 0.745\n",
      "Epoch: 149/200..  Training Loss: 0.346..  Test Loss: 0.557..  Test Accuracy: 0.745\n",
      "Epoch: 150/200..  Training Loss: 0.346..  Test Loss: 0.557..  Test Accuracy: 0.745\n",
      "Epoch: 151/200..  Training Loss: 0.345..  Test Loss: 0.557..  Test Accuracy: 0.744\n",
      "Epoch: 152/200..  Training Loss: 0.345..  Test Loss: 0.557..  Test Accuracy: 0.744\n",
      "Epoch: 153/200..  Training Loss: 0.345..  Test Loss: 0.557..  Test Accuracy: 0.743\n",
      "Epoch: 154/200..  Training Loss: 0.344..  Test Loss: 0.557..  Test Accuracy: 0.743\n",
      "Epoch: 155/200..  Training Loss: 0.344..  Test Loss: 0.557..  Test Accuracy: 0.745\n",
      "Epoch: 156/200..  Training Loss: 0.343..  Test Loss: 0.557..  Test Accuracy: 0.745\n",
      "Epoch: 157/200..  Training Loss: 0.343..  Test Loss: 0.557..  Test Accuracy: 0.745\n",
      "Epoch: 158/200..  Training Loss: 0.343..  Test Loss: 0.557..  Test Accuracy: 0.745\n",
      "Epoch: 159/200..  Training Loss: 0.342..  Test Loss: 0.557..  Test Accuracy: 0.746\n",
      "Epoch: 160/200..  Training Loss: 0.342..  Test Loss: 0.557..  Test Accuracy: 0.746\n",
      "Epoch: 161/200..  Training Loss: 0.342..  Test Loss: 0.557..  Test Accuracy: 0.746\n",
      "Epoch: 162/200..  Training Loss: 0.342..  Test Loss: 0.557..  Test Accuracy: 0.746\n",
      "Epoch: 163/200..  Training Loss: 0.341..  Test Loss: 0.556..  Test Accuracy: 0.746\n",
      "Epoch: 164/200..  Training Loss: 0.341..  Test Loss: 0.556..  Test Accuracy: 0.746\n",
      "Epoch: 165/200..  Training Loss: 0.341..  Test Loss: 0.556..  Test Accuracy: 0.746\n",
      "Epoch: 166/200..  Training Loss: 0.340..  Test Loss: 0.556..  Test Accuracy: 0.746\n",
      "Epoch: 167/200..  Training Loss: 0.340..  Test Loss: 0.556..  Test Accuracy: 0.746\n",
      "Epoch: 168/200..  Training Loss: 0.340..  Test Loss: 0.556..  Test Accuracy: 0.746\n",
      "Epoch: 169/200..  Training Loss: 0.340..  Test Loss: 0.556..  Test Accuracy: 0.745\n",
      "Epoch: 170/200..  Training Loss: 0.339..  Test Loss: 0.556..  Test Accuracy: 0.746\n",
      "Epoch: 171/200..  Training Loss: 0.339..  Test Loss: 0.556..  Test Accuracy: 0.746\n",
      "Epoch: 172/200..  Training Loss: 0.339..  Test Loss: 0.556..  Test Accuracy: 0.746\n",
      "Epoch: 173/200..  Training Loss: 0.338..  Test Loss: 0.556..  Test Accuracy: 0.746\n",
      "Epoch: 174/200..  Training Loss: 0.338..  Test Loss: 0.556..  Test Accuracy: 0.746\n",
      "Epoch: 175/200..  Training Loss: 0.338..  Test Loss: 0.556..  Test Accuracy: 0.746\n",
      "Epoch: 176/200..  Training Loss: 0.338..  Test Loss: 0.556..  Test Accuracy: 0.746\n",
      "Epoch: 177/200..  Training Loss: 0.338..  Test Loss: 0.556..  Test Accuracy: 0.746\n",
      "Epoch: 178/200..  Training Loss: 0.337..  Test Loss: 0.556..  Test Accuracy: 0.746\n",
      "Epoch: 179/200..  Training Loss: 0.337..  Test Loss: 0.556..  Test Accuracy: 0.746\n",
      "Epoch: 180/200..  Training Loss: 0.337..  Test Loss: 0.556..  Test Accuracy: 0.746\n",
      "Epoch: 181/200..  Training Loss: 0.337..  Test Loss: 0.556..  Test Accuracy: 0.746\n",
      "Epoch: 182/200..  Training Loss: 0.336..  Test Loss: 0.556..  Test Accuracy: 0.747\n",
      "Epoch: 183/200..  Training Loss: 0.336..  Test Loss: 0.556..  Test Accuracy: 0.746\n",
      "Epoch: 184/200..  Training Loss: 0.336..  Test Loss: 0.556..  Test Accuracy: 0.746\n",
      "Epoch: 185/200..  Training Loss: 0.336..  Test Loss: 0.556..  Test Accuracy: 0.746\n",
      "Epoch: 186/200..  Training Loss: 0.335..  Test Loss: 0.556..  Test Accuracy: 0.746\n",
      "Epoch: 187/200..  Training Loss: 0.335..  Test Loss: 0.556..  Test Accuracy: 0.746\n",
      "Epoch: 188/200..  Training Loss: 0.335..  Test Loss: 0.556..  Test Accuracy: 0.747\n",
      "Epoch: 189/200..  Training Loss: 0.335..  Test Loss: 0.556..  Test Accuracy: 0.747\n",
      "Epoch: 190/200..  Training Loss: 0.335..  Test Loss: 0.556..  Test Accuracy: 0.747\n",
      "Epoch: 191/200..  Training Loss: 0.334..  Test Loss: 0.556..  Test Accuracy: 0.747\n",
      "Epoch: 192/200..  Training Loss: 0.334..  Test Loss: 0.556..  Test Accuracy: 0.747\n",
      "Epoch: 193/200..  Training Loss: 0.334..  Test Loss: 0.556..  Test Accuracy: 0.747\n",
      "Epoch: 194/200..  Training Loss: 0.334..  Test Loss: 0.556..  Test Accuracy: 0.747\n",
      "Epoch: 195/200..  Training Loss: 0.334..  Test Loss: 0.556..  Test Accuracy: 0.747\n",
      "Epoch: 196/200..  Training Loss: 0.334..  Test Loss: 0.556..  Test Accuracy: 0.747\n",
      "Epoch: 197/200..  Training Loss: 0.333..  Test Loss: 0.556..  Test Accuracy: 0.747\n",
      "Epoch: 198/200..  Training Loss: 0.333..  Test Loss: 0.556..  Test Accuracy: 0.747\n",
      "Epoch: 199/200..  Training Loss: 0.333..  Test Loss: 0.556..  Test Accuracy: 0.747\n",
      "Epoch: 200/200..  Training Loss: 0.333..  Test Loss: 0.556..  Test Accuracy: 0.746\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "for e in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model.forward(x_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    train_loss = loss.item()\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        log_ps = model(x_test)\n",
    "        test_loss = criterion(log_ps, y_test)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        ps = torch.exp(log_ps)\n",
    "        top_p, top_class = ps.topk(1, dim=1)*\n",
    "        equals = top_class == y_test.view(*top_class.shape)\n",
    "        test_accuracy = torch.mean(equals.float())\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    print(f\"Epoch: {e+1}/{epochs}.. \",\n",
    "          f\"Training Loss: {train_loss:.3f}.. \",\n",
    "          f\"Test Loss: {test_loss:.3f}.. \",\n",
    "          f\"Test Accuracy: {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faba8d43-4e78-463d-8213-b75c7fdd2593",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "1. PyTorch TF-IDF:\n",
    "https://medium.com/swlh/text-classification-using-scikit-learn-pytorch-and-tensorflow-a3350808f9f7\n",
    "\n",
    "2. PyTorch loss function for binary class:\n",
    "https://discuss.pytorch.org/t/runtimeerror-expected-object-of-scalar-type-long-but-got-scalar-type-float-when-using-crossentropyloss/30542\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
